{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Í∏∞Î≥∏ train ÌååÏùº Ìò∏Ï∂ú\n",
    "- Í∏∞Î≥∏Ï†ÅÏù∏ train ÌååÏùºÏùò ÏÑ±Îä•ÏùÑ ÌôïÏù∏ÌïòÍ∏∞ ÏúÑÌï¥ Ï†úÏûë. \n",
    "- test_pearson() score: 0.81\n",
    "- base, 80% Ï¶ùÍ∞Ä, MSE score, 3 times: 0.9\n",
    "- Ï∂îÍ∞Ä 500 step warmup: 0.91"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'0.8, base'   code   lightning_logs   output.csv     wandb\n",
      " Readme.md    data   model.pt\t      output_1.csv\n",
      "Namespace(batch_size=32, continue_train=False, dev_path='./data/dev.csv', learning_rate=1e-05, max_epoch=3, model_name='klue/roberta-base', predict_path='./data/test.csv', project_name='0.8, base', shuffle=True, test_path='./data/dev.csv', train=True, train_path='./data/train.csv')\n",
      "Train mode\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkms7530\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.14.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20230414_012009-x90663hi\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mcrisp-thunder-10\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/kms7530/0.8%2C%20base\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/kms7530/0.8%2C%20base/runs/x90663hi\u001b[0m\n",
      "Some weights of the model checkpoint at klue/roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "tokenizing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9324/9324 [00:05<00:00, 1810.91it/s]\n",
      "tokenizing-idx: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9324/9324 [00:00<00:00, 543207.64it/s]\n",
      "tokenizing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 550/550 [00:00<00:00, 2987.72it/s]\n",
      "tokenizing-idx: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 550/550 [00:00<00:00, 723155.86it/s]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type                             | Params\n",
      "---------------------------------------------------------------\n",
      "0 | plm       | RobertaForSequenceClassification | 110 M \n",
      "1 | loss_func | MSELoss                          | 0     \n",
      "---------------------------------------------------------------\n",
      "110 M     Trainable params\n",
      "0         Non-trainable params\n",
      "110 M     Total params\n",
      "442.476   Total estimated model params size (MB)\n",
      "Sanity Checking: 0it [00:00, ?it/s]/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 525/525 [08:08<00:00,  1.07it/s, v_num=63hi]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/18 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/18 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|‚ñà                  | 1/18 [00:00<00:00, 68.92it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|‚ñà‚ñà                 | 2/18 [00:00<00:00, 62.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|‚ñà‚ñà‚ñà‚ñè               | 3/18 [00:00<00:00, 61.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|‚ñà‚ñà‚ñà‚ñà‚ñè              | 4/18 [00:00<00:01, 12.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé             | 5/18 [00:00<00:01,  8.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé            | 6/18 [00:00<00:01,  6.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç           | 7/18 [00:01<00:01,  5.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç          | 8/18 [00:01<00:01,  5.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå         | 9/18 [00:01<00:01,  5.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà        | 10/18 [00:02<00:01,  4.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà       | 11/18 [00:02<00:01,  4.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà      | 12/18 [00:02<00:01,  4.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà     | 13/18 [00:02<00:01,  4.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 14/18 [00:03<00:00,  4.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 15/18 [00:03<00:00,  4.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/18 [00:03<00:00,  4.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 17/18 [00:04<00:00,  4.07it/s]\u001b[A\n",
      "Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 525/525 [08:13<00:00,  1.06it/s, v_num=63hi]\u001b[A\n",
      "Epoch 1:  47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç          | 249/525 [03:52<04:17,  1.07it/s, v_num=63hi]\u001b[A"
     ]
    }
   ],
   "source": [
    "!ls\n",
    "!python code/Minseok/main.py --train=True --model_name=\"klue/roberta-base\" --project_name=\"0.8, base\" --batch_size=32"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'0.8, base'   code   lightning_logs   output.csv     wandb\n",
      " Readme.md    data   model.pt\t      output_1.csv\n",
      "Namespace(batch_size=16, dev_path='./data/dev.csv', learning_rate=1e-05, max_epoch=3, model_name='klue/roberta-base', predict_path='./data/test.csv', project_name='', shuffle=True, test_path='./data/dev.csv', train=False, train_path='./data/train.csv')\n",
      "Inference mode\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:67: UserWarning: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "  warning_cache.warn(\n",
      "tokenizing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 550/550 [00:00<00:00, 2819.78it/s]\n",
      "tokenizing-idx: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 550/550 [00:00<00:00, 828021.25it/s]\n",
      "tokenizing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1100/1100 [00:00<00:00, 2885.46it/s]\n",
      "tokenizing-idx: 0it [00:00, ?it/s]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, predict_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Predicting DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 69/69 [00:11<00:00,  6.04it/s]\n"
     ]
    }
   ],
   "source": [
    "!ls\n",
    "!python code/Minseok/main.py --train=False --model_name=\"klue/roberta-base\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
