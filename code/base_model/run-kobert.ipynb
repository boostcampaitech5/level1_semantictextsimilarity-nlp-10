{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 기본 train 파일 호출\n",
    "- 기본적인 train 파일의 성능을 확인하기 위해 제작. \n",
    "- test_pearson() score: 0.81\n",
    "- base, 80% 증가, MSE score, 3 times: 0.9\n",
    "- 추가 500 step warmup: 0.91"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Apr 15 02:09:31 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.80.02    Driver Version: 450.80.02    CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-PCIE...  Off  | 00000000:00:05.0 Off |                  Off |\n",
      "| N/A   41C    P0    39W / 250W |   2417MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install wandb -qU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kobert_tokenizer\n",
      "  Cloning https://github.com/SKTBrain/KoBERT.git to /tmp/pip-install-h0oc4e4l/kobert-tokenizer_dc9b69665c3e41d48cebd08eaff98276\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/SKTBrain/KoBERT.git /tmp/pip-install-h0oc4e4l/kobert-tokenizer_dc9b69665c3e41d48cebd08eaff98276\n",
      "  Resolved https://github.com/SKTBrain/KoBERT.git to commit 47a69af87928fc24e20f571fe10c3cc9dd9af9a3\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[?25hRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.8/site-packages (0.1.98)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#kobert 설치\n",
    "!pip install 'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf'\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 파일경로 수정이 필요할 수 있음 (base_2-kobert.py 파일도 확인 필요)\n",
    "\n",
    "### base_2-kobert.py 변경점\n",
    "dataset, dataloader, model 수정\n",
    "버그로 wandb의 부분을 개인 project로 변경, 슬랙 알람 부분을 주석처리\n",
    "sweep_config\n",
    "\n",
    "\n",
    "--entity_name 부분을 바꿔주세요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_2-kobert.py\n",
      "base_2.py\n",
      "base_2_no_sweep.py\n",
      "inference-kobert.ipynb\n",
      "main.py\n",
      "model_bat_16_lr_1.5666095706164133e-05.pt\n",
      "model_bat_16_lr_1e-05.pt\n",
      "model_bat_16_lr_1e-06.pt\n",
      "model_bat_16_lr_2.285495144038207e-05.pt\n",
      "model_bat_16_lr_2.4746298375942016e-05.pt\n",
      "model_bat_16_lr_2.549471592038539e-05.pt\n",
      "model_bat_16_lr_2.9508704693807833e-05.pt\n",
      "model_bat_16_lr_2e-05.pt\n",
      "model_bat_16_lr_3.8266306866319894e-05.pt\n",
      "model_bat_16_lr_3e-05.pt\n",
      "run-kobert.ipynb\n",
      "run.ipynb\n",
      "train-Copy-kobert.ipynb\n",
      "wandb\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhmc123123\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /opt/ml/.netrc\n",
      "Namespace(batch_size=16, dev_path='~/data/dev.csv', entity_name='hmc123123', learning_rate=1e-05, max_epoch=1, model_name='skt/kobert-base-v1', predict_path='~/data/test.csv', project_name='sts', shuffle=True, sweeps_cnt='1', test_path='~/data/dev.csv', train='True', train_path='~/data/train.csv')\n",
      "Train mode\n",
      "Create sweep with ID: 30sb085l\n",
      "Sweep URL: https://wandb.ai/hmc123123/sts/sweeps/30sb085l\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: x6puuv7e with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 1.4015386872572962e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.14.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/opt/ml/code/base_model/wandb/run-20230415_021348-x6puuv7e\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mneat-sweep-1\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/hmc123123/sts\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/hmc123123/sts/sweeps/30sb085l\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/hmc123123/sts/runs/x6puuv7e\u001b[0m\n",
      "Global seed set to 10\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'XLNetTokenizer'. \n",
      "The class this function is called from is 'KoBERTTokenizer'.\n",
      "/opt/conda/lib/python3.8/site-packages/pytorch_lightning/loggers/wandb.py:395: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "tokenizing:   0%|                                     | 0/16783 [00:00<?, ?it/s]/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "tokenizing: 100%|███████████████████████| 16783/16783 [00:09<00:00, 1774.37it/s]\n",
      "tokenizing:   0%|                                       | 0/550 [00:00<?, ?it/s]/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "tokenizing: 100%|███████████████████████████| 550/550 [00:00<00:00, 1751.96it/s]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type      | Params\n",
      "----------------------------------------\n",
      "0 | bert      | BertModel | 92.2 M\n",
      "1 | W         | Linear    | 769   \n",
      "2 | loss_func | MSELoss   | 0     \n",
      "----------------------------------------\n",
      "92.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "92.2 M    Total params\n",
      "368.751   Total estimated model params size (MB)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lr' was locked by 'sweep' (ignored update).\n",
      "Sanity Checking: 0it [00:00, ?it/s]/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Epoch 0: 100%|██████████████████| 2098/2098 [02:53<00:00, 12.08it/s, v_num=uv7e]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/69 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/69 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|▎                  | 1/69 [00:00<00:00, 71.38it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▌                  | 2/69 [00:00<00:00, 71.17it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▊                  | 3/69 [00:00<00:00, 71.00it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                  | 4/69 [00:00<00:00, 70.69it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|█▍                 | 5/69 [00:00<00:00, 70.74it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|█▋                 | 6/69 [00:00<00:00, 70.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▉                 | 7/69 [00:00<00:00, 70.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|██▏                | 8/69 [00:00<00:00, 67.31it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▍                | 9/69 [00:00<00:00, 63.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|██▌               | 10/69 [00:00<00:00, 60.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  16%|██▊               | 11/69 [00:00<00:01, 57.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|███▏              | 12/69 [00:00<00:01, 56.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|███▍              | 13/69 [00:00<00:01, 54.92it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▋              | 14/69 [00:00<00:01, 53.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▉              | 15/69 [00:00<00:01, 52.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|████▏             | 16/69 [00:00<00:01, 52.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▍             | 17/69 [00:00<00:01, 51.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|████▋             | 18/69 [00:00<00:01, 50.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|████▉             | 19/69 [00:00<00:00, 50.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|█████▏            | 20/69 [00:00<00:00, 49.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|█████▍            | 21/69 [00:00<00:00, 49.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▋            | 22/69 [00:00<00:00, 49.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|██████            | 23/69 [00:00<00:00, 48.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|██████▎           | 24/69 [00:00<00:00, 48.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████▌           | 25/69 [00:00<00:00, 48.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▊           | 26/69 [00:00<00:00, 47.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|███████           | 27/69 [00:00<00:00, 47.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|███████▎          | 28/69 [00:00<00:00, 47.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|███████▌          | 29/69 [00:00<00:00, 47.27it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▊          | 30/69 [00:00<00:00, 47.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|████████          | 31/69 [00:00<00:00, 46.93it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|████████▎         | 32/69 [00:00<00:00, 46.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████▌         | 33/69 [00:00<00:00, 46.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  49%|████████▊         | 34/69 [00:00<00:00, 46.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  51%|█████████▏        | 35/69 [00:00<00:00, 46.33it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|█████████▍        | 36/69 [00:00<00:00, 46.21it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████▋        | 37/69 [00:00<00:00, 46.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▉        | 38/69 [00:00<00:00, 45.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|██████████▏       | 39/69 [00:00<00:00, 45.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|██████████▍       | 40/69 [00:00<00:00, 45.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|██████████▋       | 41/69 [00:00<00:00, 45.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▉       | 42/69 [00:00<00:00, 45.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|███████████▏      | 43/69 [00:00<00:00, 45.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|███████████▍      | 44/69 [00:00<00:00, 45.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|███████████▋      | 45/69 [00:00<00:00, 45.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|████████████      | 46/69 [00:01<00:00, 45.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|████████████▎     | 47/69 [00:01<00:00, 45.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|████████████▌     | 48/69 [00:01<00:00, 45.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|████████████▊     | 49/69 [00:01<00:00, 45.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|█████████████     | 50/69 [00:01<00:00, 45.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|█████████████▎    | 51/69 [00:01<00:00, 45.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|█████████████▌    | 52/69 [00:01<00:00, 44.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|█████████████▊    | 53/69 [00:01<00:00, 44.92it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|██████████████    | 54/69 [00:01<00:00, 44.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|██████████████▎   | 55/69 [00:01<00:00, 44.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|██████████████▌   | 56/69 [00:01<00:00, 44.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|██████████████▊   | 57/69 [00:01<00:00, 44.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  84%|███████████████▏  | 58/69 [00:01<00:00, 44.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|███████████████▍  | 59/69 [00:01<00:00, 44.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|███████████████▋  | 60/69 [00:01<00:00, 44.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|███████████████▉  | 61/69 [00:01<00:00, 44.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|████████████████▏ | 62/69 [00:01<00:00, 44.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|████████████████▍ | 63/69 [00:01<00:00, 44.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|████████████████▋ | 64/69 [00:01<00:00, 44.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|████████████████▉ | 65/69 [00:01<00:00, 44.45it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|█████████████████▏| 66/69 [00:01<00:00, 44.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|█████████████████▍| 67/69 [00:01<00:00, 44.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|█████████████████▋| 68/69 [00:01<00:00, 44.36it/s]\u001b[A\n",
      "Epoch 0: 100%|██████████████████| 2098/2098 [02:55<00:00, 11.97it/s, v_num=uv7e]\u001b[A\n",
      "Epoch 0: 100%|██████████████████| 2098/2098 [02:55<00:00, 11.97it/s, v_num=uv7e]\u001b[A`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|██████████████████| 2098/2098 [02:57<00:00, 11.79it/s, v_num=uv7e]\n",
      "tokenizing:   0%|                                       | 0/550 [00:00<?, ?it/s]/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "tokenizing: 100%|███████████████████████████| 550/550 [00:00<00:00, 1714.66it/s]\n",
      "tokenizing:   0%|                                      | 0/1100 [00:00<?, ?it/s]/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "tokenizing: 100%|█████████████████████████| 1100/1100 [00:00<00:00, 1789.07it/s]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lr' was locked by 'sweep' (ignored update).\n",
      "/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Testing DataLoader 0: 100%|█████████████████████| 69/69 [00:01<00:00, 42.59it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "      test_pearson          0.8608465194702148\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        test_pearson ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train_loss ▇█▅▅▅▄▁▃▄▄▃▂▃▄▂▂▂▂▁▃▁▁▁▄▄▁▁▂▂▂▂▃▂▁▂▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            val_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         val_pearson ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        test_pearson 0.86085\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train_loss 0.32859\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/global_step 2098\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            val_loss 0.57102\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         val_pearson 0.85583\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mneat-sweep-1\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/hmc123123/sts/runs/x6puuv7e\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230415_021348-x6puuv7e/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: opxoqwax with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 4.1798643542216866e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.14.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/opt/ml/code/base_model/wandb/run-20230415_021718-opxoqwax\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mfanciful-sweep-2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/hmc123123/sts\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/hmc123123/sts/sweeps/30sb085l\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/hmc123123/sts/runs/opxoqwax\u001b[0m\n",
      "Global seed set to 10\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'XLNetTokenizer'. \n",
      "The class this function is called from is 'KoBERTTokenizer'.\n",
      "/opt/conda/lib/python3.8/site-packages/pytorch_lightning/loggers/wandb.py:395: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "tokenizing:   0%|                                     | 0/16783 [00:00<?, ?it/s]/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "tokenizing:  98%|██████████████████████▍| 16405/16783 [00:09<00:00, 1784.51it/s]^C\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n",
      "tokenizing:  99%|██████████████████████▋| 16558/16783 [00:09<00:00, 1697.16it/s]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!ls\n",
    "# !python code/base_model/base_2.py --train=True --model_name=\"skt/kobert-base-v1\" --sweeps_cnt=3 #기존버전\n",
    "!python ~/code/base_model/base_2-kobert.py --train=True --model_name=\"skt/kobert-base-v1\" --sweeps_cnt=1 --max_epoch=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'0.8, base'   code   lightning_logs   output.csv     wandb\n",
      " Readme.md    data   model.pt\t      output_1.csv\n",
      "Namespace(batch_size=16, dev_path='./data/dev.csv', learning_rate=1e-05, max_epoch=3, model_name='klue/roberta-base', predict_path='./data/test.csv', project_name='', shuffle=True, test_path='./data/dev.csv', train=False, train_path='./data/train.csv')\n",
      "Inference mode\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:67: UserWarning: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "  warning_cache.warn(\n",
      "tokenizing: 100%|███████████████████████████| 550/550 [00:00<00:00, 2819.78it/s]\n",
      "tokenizing-idx: 100%|█████████████████████| 550/550 [00:00<00:00, 828021.25it/s]\n",
      "tokenizing: 100%|█████████████████████████| 1100/1100 [00:00<00:00, 2885.46it/s]\n",
      "tokenizing-idx: 0it [00:00, ?it/s]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, predict_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Predicting DataLoader 0: 100%|██████████████████| 69/69 [00:11<00:00,  6.04it/s]\n"
     ]
    }
   ],
   "source": [
    "!ls\n",
    "!python ~/code/base_model/base_2-kobert.py --train=False --model_name=\"skt/kobert-base-v1\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
